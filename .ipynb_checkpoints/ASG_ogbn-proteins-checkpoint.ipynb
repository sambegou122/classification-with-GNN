{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51fcbf8",
   "metadata": {},
   "source": [
    "## IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76971d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sambe\\anaconda3\\lib\\site-packages\\torch_geometric\\typing.py:71: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] La procédure spécifiée est introuvable\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
    "from torch.nn import LayerNorm, Linear, ReLU\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.loader import RandomNodeLoader\n",
    "from torch_geometric.nn import DeepGCNLayer, GENConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85493648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Download and process data at './dataset/ogbg_molhiv/'\n",
    "ogbnproteins_dataset = PygNodePropPredDataset('ogbn-proteins', root='../data')\n",
    "ogbnarxiv_dataset = PygNodePropPredDataset(name='ogbn-arxiv', root= '../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db4ddb9",
   "metadata": {},
   "source": [
    "## Prepare DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f2b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataLoader For ogbnproteins dataset\n",
    "from torch_geometric.utils import scatter\n",
    "splitted_idx = ogbnproteins_dataset.get_idx_split()\n",
    "data = ogbnproteins_dataset[0]\n",
    "\n",
    "data.y = data.y.to(torch.float)\n",
    "\n",
    "# Initialize features of nodes by aggregating edge features.\n",
    "row, col = data.edge_index\n",
    "data.x = scatter(data.edge_attr, col, dim_size=data.num_nodes, reduce='sum')\n",
    "\n",
    "# Set split indices to masks.\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    mask[splitted_idx[split]] = True\n",
    "    data[f'{split}_mask'] = mask\n",
    "    \n",
    "\n",
    "train_loader_proteins = DataLoader(data[\"train_mask\"], batch_size=32, shuffle=True)\n",
    "valid_loader_proteins = DataLoader(data[\"valid_mask\"], batch_size=32, shuffle=False)\n",
    "test_loader_proteins = DataLoader(data[\"test_mask\"], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84455552",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataLoader For ogbnarxiv dataset\n",
    "splitted_idx = ogbnarxiv_dataset.get_idx_split()\n",
    "data = ogbnarxiv_dataset[0]\n",
    "\n",
    "data.y = data.y.to(torch.float)\n",
    "\n",
    "# Set split indices to masks.\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    mask[splitted_idx[split]] = True\n",
    "    data[f'{split}_mask'] = mask\n",
    "    \n",
    "\n",
    "train_loader_arxiv = DataLoader(data[\"train_mask\"], batch_size=32, shuffle=True)\n",
    "valid_loader_arxiv = DataLoader(data[\"valid_mask\"], batch_size=32, shuffle=False)\n",
    "test_loader_arxiv = DataLoader(data[\"test_mask\"], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3a3d42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  ..., False, False, False])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6437ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_evaluate_model(model, loaders, loss,optimizer):\n",
    "    train_loader = loaders[\"train\"]\n",
    "    valid_loader = loaders[\"valid\"]\n",
    "    test_loader = loaders[\"test\"]\n",
    "    \n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            print(batch.batch_size)\n",
    "            out = model(batch.x, batch.edge_index)[:batch.batch_size]\n",
    "            y = batch.y[:batch.batch_size].view(-1).to(torch.long)\n",
    "            loss = loss(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch: {epoch:02d}, Iteration: {i}, Loss: {loss:.4f}, '\n",
    "                      f's/iter: {time.perf_counter() - start:.6f}')\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "\n",
    "        total_correct = total_examples = 0\n",
    "        for i, batch in enumerate(loader):\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index)[:batch.batch_size]\n",
    "            pred = out.argmax(dim=-1)\n",
    "            y = batch.y[:batch.batch_size].view(-1).to(torch.long)\n",
    "\n",
    "            total_correct += int((pred == y).sum())\n",
    "            total_examples += y.size(0)\n",
    "\n",
    "        return total_correct / total_examples\n",
    "    \n",
    "    for epoch in range(1, 101):\n",
    "        train()\n",
    "    result = {}\n",
    "    result[\"train_accuracy\"] = test(train_loader)\n",
    "    result[\"valid_accuracy\"] = test(valid_loader)\n",
    "    result[\"test_accuracy\"] = test(test_loader)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "186b50ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import Node2Vec\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
    "from torch.nn import LayerNorm, Linear, ReLU\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.loader import RandomNodeLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Node2Vec(\n",
    "    data.edge_index,\n",
    "    embedding_dim=128,\n",
    "    walk_length=20,\n",
    "    context_size=10,\n",
    "    walks_per_node=10,\n",
    "    num_negative_samples=1,\n",
    "    p=1.0,\n",
    "    q=1.0,\n",
    "    sparse=True,\n",
    ").to(device)\n",
    "\n",
    "num_workers = 4 if sys.platform == 'linux' else 0\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=num_workers)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "loss = F.cross_entropy\n",
    "loaders = {}\n",
    "loaders[\"train\"] = train_loader_proteins\n",
    "loaders[\"valid\"] = valid_loader_proteins\n",
    "loaders[\"test\"] = test_loader_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ad75772",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11480\\2945318479.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_test_evaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11480\\2308611171.py\u001b[0m in \u001b[0;36mtrain_test_evaluate_model\u001b[1;34m(model, loaders, loss, optimizer)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m101\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train_accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11480\\2308611171.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'batch_size'"
     ]
    }
   ],
   "source": [
    "train_test_evaluate_model(model, loaders, loss,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1208ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import Node2Vec\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
    "from torch.nn import LayerNorm, Linear, ReLU\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.loader import RandomNodeLoader\n",
    "\n",
    "\n",
    "dataset = PygNodePropPredDataset('ogbn-proteins', root='../data')\n",
    "data = dataset[0]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Node2Vec(\n",
    "    data.edge_index,\n",
    "    embedding_dim=128,\n",
    "    walk_length=20,\n",
    "    context_size=10,\n",
    "    walks_per_node=10,\n",
    "    num_negative_samples=1,\n",
    "    p=1.0,\n",
    "    q=1.0,\n",
    "    sparse=True,\n",
    ").to(device)\n",
    "\n",
    "num_workers = 4 if sys.platform == 'linux' else 0\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=num_workers)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    z = model()\n",
    "    acc = model.test(\n",
    "        train_z=z[data.train_mask],\n",
    "        train_y=data.y[data.train_mask],\n",
    "        test_z=z[data.test_mask],\n",
    "        test_y=data.y[data.test_mask],\n",
    "        max_iter=150,\n",
    "    )\n",
    "    return acc\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    acc = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Acc: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-arxiv')\n",
    "splitted_idx = dataset.get_idx_split()\n",
    "data = dataset[0]\n",
    "data.node_species = None\n",
    "data.y = data.y.to(torch.float)\n",
    "\n",
    "# Initialize features of nodes by aggregating edge features.\n",
    "row, col = data.edge_index\n",
    "data.x = scatter(data.edge_attr, col, dim_size=data.num_nodes, reduce='sum')\n",
    "\n",
    "# Set split indices to masks.\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    mask[splitted_idx[split]] = True\n",
    "    data[f'{split}_mask'] = mask\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(dataset.num_features, 64, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0005)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        start = time.perf_counter()\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)[:batch.batch_size]\n",
    "        y = batch.y[:batch.batch_size].view(-1).to(torch.long)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {epoch:02d}, Iteration: {i}, Loss: {loss:.4f}, '\n",
    "                  f's/iter: {time.perf_counter() - start:.6f}')\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader: NeighborLoader, val_steps: Optional[int] = None):\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = total_examples = 0\n",
    "    for i, batch in enumerate(loader):\n",
    "        if val_steps is not None and i >= val_steps:\n",
    "            break\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.x, batch.edge_index)[:batch.batch_size]\n",
    "        pred = out.argmax(dim=-1)\n",
    "        y = batch.y[:batch.batch_size].view(-1).to(torch.long)\n",
    "\n",
    "        total_correct += int((pred == y).sum())\n",
    "        total_examples += y.size(0)\n",
    "\n",
    "    return total_correct / total_examples\n",
    "\n",
    "\n",
    "for epoch in range(1, 4):\n",
    "    train()\n",
    "    val_acc = test(val_loader, val_steps=100)\n",
    "    print(f'Val Acc: ~{val_acc:.4f}')\n",
    "\n",
    "test_acc = test(test_loader)\n",
    "print(f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "init_wandb(\n",
    "    name=f'GCN-{args.dataset}',\n",
    "    lr=args.lr,\n",
    "    epochs=args.epochs,\n",
    "    hidden_channels=args.hidden_channels,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
    "dataset = Planetoid(path, args.dataset, transform=T.NormalizeFeatures())\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if args.use_gdc:\n",
    "    transform = T.GDC(\n",
    "        self_loop_weight=1,\n",
    "        normalization_in='sym',\n",
    "        normalization_out='col',\n",
    "        diffusion_kwargs=dict(method='ppr', alpha=0.05),\n",
    "        sparsification_kwargs=dict(method='topk', k=128, dim=0),\n",
    "        exact=True,\n",
    "    )\n",
    "    data = transform(data)\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels,\n",
    "                             normalize=not args.use_gdc)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels,\n",
    "                             normalize=not args.use_gdc)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(\n",
    "    in_channels=dataset.num_features,\n",
    "    hidden_channels=args.hidden_channels,\n",
    "    out_channels=dataset.num_classes,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    dict(params=model.conv1.parameters(), weight_decay=5e-4),\n",
    "    dict(params=model.conv2.parameters(), weight_decay=0)\n",
    "], lr=args.lr)  # Only perform weight-decay on first convolution.\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index, data.edge_attr).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs\n",
    "\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "times = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n",
    "    times.append(time.time() - start)\n",
    "print(f'Median time per epoch: {torch.tensor(times).median():.4f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba47031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea99546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029bde02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ba21f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80b00b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db983c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed895e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376ab78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c11caca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14492d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a132ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8e7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec384ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c66938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd9d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a6f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c2191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7588266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ef7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d6980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefc078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2f4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8724e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96147da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9bec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de58d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c523d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
